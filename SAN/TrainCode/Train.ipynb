{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "import queue\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as multiprocessing\n",
    "\n",
    "from torch._C import _set_worker_signal_handlers, _update_worker_pids, \\\n",
    "    _remove_worker_pids, _error_if_any_worker_fails\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataloader import _DataLoaderIter\n",
    "\n",
    "from torch.utils.data.dataloader import ExceptionWrapper\n",
    "from torch.utils.data.dataloader import _use_shared_memory\n",
    "from torch.utils.data.dataloader import _worker_manager_loop\n",
    "from torch.utils.data.dataloa der import numpy_type_map\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.dataloader import pin_memory_batch\n",
    "from torch.utils.data.dataloader import _SIGCHLD_handler_set\n",
    "from torch.utils.data.dataloader import _set_SIGCHLD_handler\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import Queue as queue\n",
    "else:\n",
    "    import queue\n",
    "\n",
    "def _ms_loop(dataset, index_queue, data_queue, collate_fn, scale, seed, init_fn, worker_id):\n",
    "    global _use_shared_memory\n",
    "    _use_shared_memory = True\n",
    "    _set_worker_signal_handlers()\n",
    "\n",
    "    torch.set_num_threads(1)\n",
    "    torch.manual_seed(seed)\n",
    "    while True:\n",
    "        r = index_queue.get()\n",
    "        if r is None:\n",
    "            break\n",
    "        idx, batch_indices = r\n",
    "        try:\n",
    "            idx_scale = 0\n",
    "            if len(scale) > 1 and dataset.train:\n",
    "                idx_scale = random.randrange(0, len(scale))\n",
    "                dataset.set_scale(idx_scale)\n",
    "\n",
    "            samples = collate_fn([dataset[i] for i in batch_indices])\n",
    "            samples.append(idx_scale)\n",
    "\n",
    "        except Exception:\n",
    "            data_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n",
    "        else:\n",
    "            data_queue.put((idx, samples))\n",
    "\n",
    "class _MSDataLoaderIter(_DataLoaderIter):\n",
    "    def __init__(self, loader):\n",
    "        self.dataset = loader.dataset\n",
    "        self.scale = loader.scale\n",
    "        self.collate_fn = loader.collate_fn\n",
    "        self.batch_sampler = loader.batch_sampler\n",
    "        self.num_workers = loader.num_workers\n",
    "        self.pin_memory = loader.pin_memory and torch.cuda.is_available()\n",
    "        self.timeout = loader.timeout\n",
    "        self.done_event = threading.Event()\n",
    "\n",
    "        self.sample_iter = iter(self.batch_sampler)\n",
    "\n",
    "        if self.num_workers > 0:\n",
    "            self.worker_init_fn = loader.worker_init_fn\n",
    "            self.index_queues = [\n",
    "                multiprocessing.Queue() for _ in range(self.num_workers)\n",
    "            ]\n",
    "            self.worker_queue_idx = 0\n",
    "            self.worker_result_queue = multiprocessing.SimpleQueue()\n",
    "            self.batches_outstanding = 0\n",
    "            self.worker_pids_set = False\n",
    "            self.shutdown = False\n",
    "            self.send_idx = 0\n",
    "            self.rcvd_idx = 0\n",
    "            self.reorder_dict = {}\n",
    "\n",
    "            base_seed = torch.LongTensor(1).random_()[0]\n",
    "            self.workers = [\n",
    "                multiprocessing.Process(\n",
    "                    target=_ms_loop,\n",
    "                    args=(\n",
    "                        self.dataset,\n",
    "                        self.index_queues[i],\n",
    "                        self.worker_result_queue,\n",
    "                        self.collate_fn,\n",
    "                        self.scale,\n",
    "                        base_seed + i,\n",
    "                        self.worker_init_fn,\n",
    "                        i\n",
    "                    )\n",
    "                )\n",
    "                for i in range(self.num_workers)]\n",
    "\n",
    "            if self.pin_memory or self.timeout > 0:\n",
    "                self.data_queue = queue.Queue()\n",
    "                if self.pin_memory:\n",
    "                    maybe_device_id = torch.cuda.current_device()\n",
    "                else:\n",
    "                    # do not initialize cuda context if not necessary\n",
    "                    maybe_device_id = None\n",
    "                self.worker_manager_thread = threading.Thread(\n",
    "                    target=_worker_manager_loop,\n",
    "                    args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n",
    "                          maybe_device_id))\n",
    "                self.worker_manager_thread.daemon = True\n",
    "                self.worker_manager_thread.start()\n",
    "            else:\n",
    "                self.data_queue = self.worker_result_queue\n",
    "\n",
    "            for w in self.workers:\n",
    "                w.daemon = True  # ensure that the worker exits on process exit\n",
    "                w.start()\n",
    "\n",
    "            _update_worker_pids(id(self), tuple(w.pid for w in self.workers))\n",
    "            _set_SIGCHLD_handler()\n",
    "            self.worker_pids_set = True\n",
    "\n",
    "            # prime the prefetch loop\n",
    "            for _ in range(2 * self.num_workers):\n",
    "                self._put_indices()\n",
    "\n",
    "class MSDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self, args, dataset, batch_size=1, shuffle=False,\n",
    "        sampler=None, batch_sampler=None,\n",
    "        collate_fn=default_collate, pin_memory=False, drop_last=False,\n",
    "        timeout=0, worker_init_fn=None):\n",
    "\n",
    "        super(MSDataLoader, self).__init__(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "            sampler=sampler, batch_sampler=batch_sampler,\n",
    "            num_workers=args.n_threads, collate_fn=collate_fn,\n",
    "            pin_memory=pin_memory, drop_last=drop_last,\n",
    "            timeout=timeout, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        self.scale = args.scale\n",
    "\n",
    "    def __iter__(self):\n",
    "        return _MSDataLoaderIter(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lrs\n",
    "\n",
    "class timer():\n",
    "    def __init__(self):\n",
    "        self.acc = 0\n",
    "        self.tic()\n",
    "\n",
    "    def tic(self):\n",
    "        self.t0 = time.time()\n",
    "\n",
    "    def toc(self):\n",
    "        return time.time() - self.t0\n",
    "\n",
    "    def hold(self):\n",
    "        self.acc += self.toc()\n",
    "\n",
    "    def release(self):\n",
    "        ret = self.acc\n",
    "        self.acc = 0\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def reset(self):\n",
    "        self.acc = 0\n",
    "\n",
    "class checkpoint():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.ok = True\n",
    "        self.log = torch.Tensor()\n",
    "        now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "        if args.load == '.':\n",
    "            if args.save == '.':\n",
    "                args.save = now\n",
    "            self.dir = '../experiment/' + args.save\n",
    "        else:\n",
    "            self.dir = '../experiment/' + args.load\n",
    "            if not os.path.exists(self.dir):\n",
    "                args.load = '.'\n",
    "            else:\n",
    "                self.log = torch.load(self.dir + '/psnr_log.pt')\n",
    "                print('Continue from epoch {}...'.format(len(self.log)))\n",
    "\n",
    "        if args.reset:\n",
    "            os.system('rm -rf ' + self.dir)\n",
    "            args.load = '.'\n",
    "\n",
    "        def _make_dir(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        _make_dir(self.dir)\n",
    "        _make_dir(self.dir + '/model')\n",
    "        _make_dir(self.dir + '/results')\n",
    "\n",
    "        open_type = 'a' if os.path.exists(self.dir + '/log.txt') else 'w'\n",
    "        self.log_file = open(self.dir + '/log.txt', open_type)\n",
    "        with open(self.dir + '/config.txt', open_type) as f:\n",
    "            f.write(now + '\\n\\n')\n",
    "            for arg in vars(args):\n",
    "                f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
    "            f.write('\\n')\n",
    "\n",
    "    def save(self, trainer, epoch, is_best=False):\n",
    "        trainer.model.save(self.dir, epoch, is_best=is_best)\n",
    "        trainer.loss.save(self.dir)\n",
    "        trainer.loss.plot_loss(self.dir, epoch)\n",
    "\n",
    "        self.plot_psnr(epoch)\n",
    "        torch.save(self.log, os.path.join(self.dir, 'psnr_log.pt'))\n",
    "        torch.save(\n",
    "            trainer.optimizer.state_dict(),\n",
    "            os.path.join(self.dir, 'optimizer.pt')\n",
    "        )\n",
    "\n",
    "    def add_log(self, log):\n",
    "        self.log = torch.cat([self.log, log])\n",
    "\n",
    "    def write_log(self, log, refresh=False):\n",
    "        print(log)\n",
    "        self.log_file.write(log + '\\n')\n",
    "        if refresh:\n",
    "            self.log_file.close()\n",
    "            self.log_file = open(self.dir + '/log.txt', 'a')\n",
    "\n",
    "    def done(self):\n",
    "        self.log_file.close()\n",
    "\n",
    "    def plot_psnr(self, epoch):\n",
    "        axis = np.linspace(1, epoch, epoch)\n",
    "        label = 'SR on {}'.format(self.args.data_test)\n",
    "        fig = plt.figure()\n",
    "        plt.title(label)\n",
    "        for idx_scale, scale in enumerate(self.args.scale):\n",
    "            plt.plot(\n",
    "                axis,\n",
    "                self.log[:, idx_scale].numpy(),\n",
    "                label='Scale {}'.format(scale)\n",
    "            )\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('PSNR')\n",
    "        plt.grid(True)\n",
    "        plt.savefig('{}/test_{}.pdf'.format(self.dir, self.args.data_test))\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_results(self, filename, save_list, scale):\n",
    "        filename = '{}/results/{}_x{}_'.format(self.dir, filename, scale)\n",
    "        postfix = ('SR', 'LR', 'HR')\n",
    "        for v, p in zip(save_list, postfix):\n",
    "            normalized = v[0].data.mul(255 / self.args.rgb_range)\n",
    "            ndarr = normalized.byte().permute(1, 2, 0).cpu().numpy()\n",
    "            misc.imsave('{}{}.png'.format(filename, p), ndarr)\n",
    "\n",
    "def quantize(img, rgb_range):\n",
    "    pixel_range = 255 / rgb_range\n",
    "    return img.mul(pixel_range).clamp(0, 255).round().div(pixel_range)\n",
    "\n",
    "def calc_psnr(sr, hr, scale, rgb_range, benchmark=False):\n",
    "    diff = (sr - hr).data.div(rgb_range)\n",
    "    shave = scale\n",
    "    if diff.size(1) > 1:\n",
    "        convert = diff.new(1, 3, 1, 1)\n",
    "        convert[0, 0, 0, 0] = 65.738\n",
    "        convert[0, 1, 0, 0] = 129.057\n",
    "        convert[0, 2, 0, 0] = 25.064\n",
    "        diff.mul_(convert).div_(256)\n",
    "        diff = diff.sum(dim=1, keepdim=True)\n",
    "    '''\n",
    "    if benchmark:\n",
    "        shave = scale\n",
    "        if diff.size(1) > 1:\n",
    "            convert = diff.new(1, 3, 1, 1)\n",
    "            convert[0, 0, 0, 0] = 65.738\n",
    "            convert[0, 1, 0, 0] = 129.057\n",
    "            convert[0, 2, 0, 0] = 25.064\n",
    "            diff.mul_(convert).div_(256)\n",
    "            diff = diff.sum(dim=1, keepdim=True)\n",
    "    else:\n",
    "        shave = scale + 6\n",
    "    '''\n",
    "    valid = diff[:, :, shave:-shave, shave:-shave]\n",
    "    mse = valid.pow(2).mean()\n",
    "\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def make_optimizer(args, my_model):\n",
    "    trainable = filter(lambda x: x.requires_grad, my_model.parameters())\n",
    "\n",
    "    if args.optimizer == 'SGD':\n",
    "        optimizer_function = optim.SGD\n",
    "        kwargs = {'momentum': args.momentum}\n",
    "    elif args.optimizer == 'ADAM':\n",
    "        optimizer_function = optim.Adam\n",
    "        kwargs = {\n",
    "            'betas': (args.beta1, args.beta2),\n",
    "            'eps': args.epsilon\n",
    "        }\n",
    "    elif args.optimizer == 'RMSprop':\n",
    "        optimizer_function = optim.RMSprop\n",
    "        kwargs = {'eps': args.epsilon}\n",
    "\n",
    "    kwargs['lr'] = args.lr\n",
    "    kwargs['weight_decay'] = args.weight_decay\n",
    "    \n",
    "    return optimizer_function(trainable, **kwargs)\n",
    "\n",
    "def make_scheduler(args, my_optimizer):\n",
    "    if args.decay_type == 'step':\n",
    "        scheduler = lrs.StepLR(\n",
    "            my_optimizer,\n",
    "            step_size=args.lr_decay,\n",
    "            gamma=args.gamma\n",
    "        )\n",
    "    elif args.decay_type.find('step') >= 0:\n",
    "        milestones = args.decay_type.split('_')\n",
    "        milestones.pop(0)\n",
    "        milestones = list(map(lambda x: int(x), milestones))\n",
    "        scheduler = lrs.MultiStepLR(\n",
    "            my_optimizer,\n",
    "            milestones=milestones,\n",
    "            gamma=args.gamma\n",
    "        )\n",
    "\n",
    "    return scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from decimal import Decimal\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, args, loader, my_model, my_loss, ckp):\n",
    "        self.args = args\n",
    "        self.scale = args.scale\n",
    "\n",
    "        self.ckp = ckp   # checkpoint\n",
    "        self.loader_train = loader.loader_train\n",
    "        self.loader_test = loader.loader_test\n",
    "        self.model = my_model\n",
    "        self.loss = my_loss\n",
    "        self.optimizer = make_optimizer(args, self.model)\n",
    "        self.scheduler = make_scheduler(args, self.optimizer)\n",
    "\n",
    "        if self.args.load != '.':\n",
    "            self.optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(ckp.dir, 'optimizer.pt'))\n",
    "            )\n",
    "            for _ in range(len(ckp.log)): self.scheduler.step()\n",
    "\n",
    "        self.error_last = 1e8\n",
    "\n",
    "    def train(self):\n",
    "        self.scheduler.step()\n",
    "        self.loss.step()\n",
    "        epoch = self.scheduler.last_epoch + 1\n",
    "        lr = self.scheduler.get_lr()[0]\n",
    "\n",
    "        self.ckp.write_log(\n",
    "            '[Epoch {}]\\tLearning rate: {:.2e}'.format(epoch, Decimal(lr))\n",
    "        )\n",
    "        self.loss.start_log()\n",
    "        self.model.train()\n",
    "\n",
    "        timer_data, timer_model = timer(), timer()\n",
    "        for batch, (lr, hr, _, idx_scale) in enumerate(self.loader_train):\n",
    "            lr, hr = self.prepare([lr, hr])\n",
    "            timer_data.hold()\n",
    "            timer_model.tic()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            sr = self.model(lr, idx_scale)\n",
    "            loss = self.loss(sr, hr)\n",
    "            if loss.item() < self.args.skip_threshold * self.error_last:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            else:\n",
    "                print('Skip this batch {}! (Loss: {})'.format(\n",
    "                    batch + 1, loss.item()\n",
    "                ))\n",
    "\n",
    "            timer_model.hold()\n",
    "\n",
    "            if (batch + 1) % self.args.print_every == 0:\n",
    "                self.ckp.write_log('[{}/{}]\\t{}\\t{:.1f}+{:.1f}s'.format(\n",
    "                    (batch + 1) * self.args.batch_size,\n",
    "                    len(self.loader_train.dataset),\n",
    "                    self.loss.display_loss(batch),\n",
    "                    timer_model.release(),\n",
    "                    timer_data.release()))\n",
    "\n",
    "            timer_data.tic()\n",
    "\n",
    "        self.loss.end_log(len(self.loader_train))\n",
    "        self.error_last = self.loss.log[-1, -1]\n",
    "\n",
    "    def test(self):\n",
    "        epoch = self.scheduler.last_epoch + 1\n",
    "        self.ckp.write_log('\\nEvaluation:')\n",
    "        self.ckp.add_log(torch.zeros(1, len(self.scale)))\n",
    "        self.model.eval()\n",
    "\n",
    "        timer_test = timer()\n",
    "        with torch.no_grad():\n",
    "            for idx_scale, scale in enumerate(self.scale):\n",
    "                eval_acc = 0\n",
    "                self.loader_test.dataset.set_scale(idx_scale)\n",
    "                tqdm_test = tqdm(self.loader_test, ncols=80)\n",
    "                for idx_img, (lr, hr, filename, _) in enumerate(tqdm_test):\n",
    "                    filename = filename[0]\n",
    "                    no_eval = (hr.nelement() == 1)\n",
    "                    if not no_eval:\n",
    "                        lr, hr = self.prepare([lr, hr])\n",
    "                    else:\n",
    "                        lr = self.prepare([lr])[0]\n",
    "\n",
    "                    sr = self.model(lr, idx_scale)\n",
    "                    sr = quantize(sr, self.args.rgb_range)\n",
    "\n",
    "                    save_list = [sr]\n",
    "                    if not no_eval:\n",
    "                        eval_acc += calc_psnr(\n",
    "                            sr, hr, scale, self.args.rgb_range,\n",
    "                            benchmark=self.loader_test.dataset.benchmark\n",
    "                        )\n",
    "                        save_list.extend([lr, hr])\n",
    "\n",
    "                    if self.args.save_results:\n",
    "                        self.ckp.save_results(filename, save_list, scale)\n",
    "\n",
    "                self.ckp.log[-1, idx_scale] = eval_acc / len(self.loader_test)\n",
    "                best = self.ckp.log.max(0)\n",
    "                self.ckp.write_log(\n",
    "                    '[{} x{}]\\tPSNR: {:.3f} (Best: {:.3f} @epoch {})'.format(\n",
    "                        self.args.data_test,\n",
    "                        scale,\n",
    "                        self.ckp.log[-1, idx_scale],\n",
    "                        best[0][idx_scale],\n",
    "                        best[1][idx_scale] + 1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.ckp.write_log(\n",
    "            'Total time: {:.2f}s\\n'.format(timer_test.toc()), refresh=True\n",
    "        )\n",
    "        if not self.args.test_only:\n",
    "            self.ckp.save(self, epoch, is_best=(best[1][0] + 1 == epoch))\n",
    "\n",
    "    def prepare(self, l, volatile=False):\n",
    "        device = torch.device('cpu' if self.args.cpu else 'cuda')\n",
    "        def _prepare(tensor):\n",
    "            if self.args.precision == 'half':\n",
    "                tensor = tensor.half()\n",
    "            return tensor.to(device)\n",
    "           \n",
    "        return [_prepare(_l) for _l in l]\n",
    "\n",
    "    def terminate(self):\n",
    "        if self.args.test_only:\n",
    "            self.test()\n",
    "            return True\n",
    "        else:\n",
    "            epoch = self.scheduler.last_epoch + 1\n",
    "            return epoch >= self.args.epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_template(args):\n",
    "    # Set the templates here\n",
    "    if args.template.find('jpeg') >= 0:\n",
    "        args.data_train = 'DIV2K_jpeg'\n",
    "        args.data_test = 'DIV2K_jpeg'\n",
    "        args.epochs = 200\n",
    "        args.lr_decay = 100\n",
    "\n",
    "    if args.template.find('EDSR_paper') >= 0:\n",
    "        args.model = 'EDSR'\n",
    "        args.n_resblocks = 32\n",
    "        args.n_feats = 256\n",
    "        args.res_scale = 0.1\n",
    "\n",
    "    if args.template.find('MDSR') >= 0:\n",
    "        args.model = 'MDSR'\n",
    "        args.patch_size = 48\n",
    "        args.epochs = 650\n",
    "\n",
    "    if args.template.find('DDBPN') >= 0:\n",
    "        args.model = 'DDBPN'\n",
    "        args.patch_size = 128\n",
    "        args.scale = '4'\n",
    "\n",
    "        args.data_test = 'Set5'\n",
    "\n",
    "        args.batch_size = 20\n",
    "        args.epochs = 1000\n",
    "        args.lr_decay = 500\n",
    "        args.gamma = 0.1\n",
    "        args.weight_decay = 1e-4\n",
    "\n",
    "        args.loss = '1*MSE'\n",
    "\n",
    "    if args.template.find('GAN') >= 0:\n",
    "        args.epochs = 200\n",
    "        args.lr = 5e-5\n",
    "        args.lr_decay = 150\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='EDSR and MDSR')\n",
    "\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "                    help='Enables debug mode')\n",
    "parser.add_argument('--template', default='.',\n",
    "                    help='You can set various templates in option.py')\n",
    "\n",
    "# Hardware specifications\n",
    "parser.add_argument('--n_threads', type=int, default=8,\n",
    "                    help='number of threads for data loading')\n",
    "parser.add_argument('--cpu', action='store_true',\n",
    "                    help='use cpu only')\n",
    "parser.add_argument('--n_GPUs', type=int, default=1,\n",
    "                    help='number of GPUs')\n",
    "parser.add_argument('--GPU_id', type=str, default=\"0\", choices=['0',\"1\",'2','3'],\n",
    "                    help='if n_GPUs==1, specify GPU index')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "\n",
    "# Data specifications\n",
    "parser.add_argument('--dir_data', type=str, default='/data/daitao/code_142/superresolution/SR_dataset',\n",
    "                    help='dataset directory')\n",
    "parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                    help='demo image directory')\n",
    "parser.add_argument('--data_train', type=str, default='DIV2K',\n",
    "                    help='train dataset name')\n",
    "parser.add_argument('--data_test', type=str, default='Set5', choices=['DIV2K','Set5','Set14'],\n",
    "                    help='test dataset name')\n",
    "parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                    help='use noisy benchmark sets')\n",
    "parser.add_argument('--n_train', type=int, default=800,\n",
    "                    help='number of training set')\n",
    "parser.add_argument('--n_val', type=int, default=5,\n",
    "                    help='number of validation set')\n",
    "parser.add_argument('--offset_val', type=int, default=5,\n",
    "                    help='validation index offest')\n",
    "parser.add_argument('--ext', type=str, default='sep_reset',choices=['sep_reset','sep'],\n",
    "                    help='dataset file extension')\n",
    "parser.add_argument('--scale', default='4',\n",
    "                    help='super resolution scale')\n",
    "parser.add_argument('--patch_size', type=int, default=192,\n",
    "                    help='output patch size')\n",
    "parser.add_argument('--rgb_range', type=int, default=255,\n",
    "                    help='maximum value of RGB')\n",
    "parser.add_argument('--n_colors', type=int, default=3,\n",
    "                    help='number of color channels to use')\n",
    "parser.add_argument('--noise', type=str, default='.',\n",
    "                    help='Gaussian noise std.')\n",
    "parser.add_argument('--chop', action='store_true',\n",
    "                    help='enable memory-efficient forward')\n",
    "\n",
    "# Model specifications\n",
    "parser.add_argument('--model', default='san',\n",
    "                    help='model name')\n",
    "\n",
    "parser.add_argument('--act', type=str, default='relu',\n",
    "                    help='activation function')\n",
    "parser.add_argument('--pre_train', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--extend', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--n_resblocks', type=int, default=10,\n",
    "                    help='number of residual blocks')\n",
    "parser.add_argument('--n_feats', type=int, default=64,\n",
    "                    help='number of feature maps')\n",
    "parser.add_argument('--res_scale', type=float, default=1,\n",
    "                    help='residual scaling')\n",
    "parser.add_argument('--shift_mean', default=True,\n",
    "                    help='subtract pixel mean from the input')\n",
    "parser.add_argument('--precision', type=str, default='single',\n",
    "                    choices=('single', 'half'),\n",
    "                    help='FP precision for test (single | half)')\n",
    "\n",
    "# Training specifications\n",
    "parser.add_argument('--reset', action='store_true',\n",
    "                    help='reset the training')\n",
    "parser.add_argument('--test_every', type=int, default=1000,\n",
    "                    help='do test per every N batches')\n",
    "parser.add_argument('--epochs', type=int, default=3000,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size', type=int, default=16,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--split_batch', type=int, default=1,\n",
    "                    help='split the batch into smaller chunks')\n",
    "parser.add_argument('--self_ensemble', action='store_true',\n",
    "                    help='use self-ensemble method for test')\n",
    "parser.add_argument('--test_only', action='store_true',\n",
    "                    help='set this option to test the model')\n",
    "parser.add_argument('--gan_k', type=int, default=1,\n",
    "                    help='k value for adversarial loss')\n",
    "\n",
    "# Optimization specifications\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay', type=int, default=50,\n",
    "                    help='learning rate decay per N epochs')\n",
    "parser.add_argument('--decay_type', type=str, default='step',\n",
    "                    help='learning rate decay type')\n",
    "parser.add_argument('--gamma', type=float, default=0.6,\n",
    "                    help='learning rate decay factor for step decay')\n",
    "parser.add_argument('--optimizer', default='ADAM',\n",
    "                    choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                    help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                    help='ADAM beta1')\n",
    "parser.add_argument('--beta2', type=float, default=0.99,\n",
    "                    help='ADAM beta2')\n",
    "parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                    help='ADAM epsilon for numerical stability')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "\n",
    "# Loss specifications\n",
    "parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                    help='loss function configuration')\n",
    "parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                    help='skipping batch that has large error')\n",
    "\n",
    "# Log specifications\n",
    "parser.add_argument('--save', type=str, default='test',\n",
    "                    help='file name to save')\n",
    "parser.add_argument('--load', type=str, default='.',\n",
    "                    help='file name to load')\n",
    "parser.add_argument('--resume', type=int, default=0,\n",
    "                    help='resume from specific checkpoint')\n",
    "parser.add_argument('--print_model', action='store_true',\n",
    "                    help='print model')\n",
    "parser.add_argument('--save_models', action='store_true',\n",
    "                    help='save all intermediate models')\n",
    "parser.add_argument('--print_every', type=int, default=100,\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save_results', action='store_true',\n",
    "                    help='save output results')\n",
    "\n",
    "# options for residual group and feature channel reduction\n",
    "parser.add_argument('--n_resgroups', type=int, default=20,\n",
    "                    help='number of residual groups')\n",
    "parser.add_argument('--reduction', type=int, default=16,\n",
    "                    help='number of feature maps reduction')\n",
    "# options for test\n",
    "parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                    help='dataset directory for testing')\n",
    "parser.add_argument('--testset', type=str, default='Set5',\n",
    "                    help='dataset name for testing')\n",
    "\n",
    "args = parser.parse_args(args = ['--model', 'san', '--data_test', 'MyImage', '--save', 'Set14x2', \n",
    "        '--scale', '2', '--n_resgroups', '20', '--n_resblocks', '10', '--n_feats', '64',\n",
    "        '--reset', '--chop', '--save_results', '--test_only', '--testpath', '../LR/LRBI',\n",
    "        '--testset', 'Set14', '--pre_train', '../model/SAN_BI2X.pt', '--n_threads', '0'])template.set_template(args)\n",
    "args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "\n",
    "if args.epochs == 0:\n",
    "    args.epochs = 1e8\n",
    "\n",
    "for arg in vars(args):\n",
    "    if vars(args)[arg] == 'True':\n",
    "        vars(args)[arg] = True\n",
    "    elif vars(args)[arg] == 'False':\n",
    "        vars(args)[arg] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import utility\n",
    "import data\n",
    "import model\n",
    "import loss\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "checkpoint = checkpoint(args)\n",
    "\n",
    "if checkpoint.ok:\n",
    "    loader = data.Data(args)\n",
    "    model = model.Model(args, checkpoint)\n",
    "    loss = loss.Loss(args, checkpoint) if not args.test_only else None\n",
    "    t = Trainer(args, loader, model, loss, checkpoint)\n",
    "    while not t.terminate():\n",
    "        t.train()\n",
    "        t.test()\n",
    "\n",
    "    checkpoint.done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
